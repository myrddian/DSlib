{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5148: Assessment 1\n",
    "## Distributed Databases and Big Data\n",
    "\n",
    "\n",
    "### Student Name: Enzo Reyes\n",
    "\n",
    "### Student Number: 21879818\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Approach\n",
    "\n",
    "For this assignment I wanted to try a re-usable approach, where upon each task wasn't an isolated unit of item/work, but it used a framework, as such things took a different approach from the tutorials.\n",
    "\n",
    "Each part of function was made to operate as if it where working in a pipeline, on certain tasks I have a _kernel_ function which is that data pipeline that is run on each processor.\n",
    "\n",
    "Also I wanted to to thing in which there was no possible race condition, as such most of the time the way I wrote things was based on determining if there was a race condition and making sure that the data was either copied if it was manipulated or isolated to one processor. Luckily a lot of this depended a lot on the Scheduler.\n",
    "\n",
    "The other thing was that I didn't operate on the whole data structure, each function only works on its mailbox and reads values from the shared data frame as if it were read-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import csv\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#Define the number of workers\n",
    "__N_WORKERS__ = 4  # Working with 4 CPU\n",
    "__CRASH_DATA__ = \"2018_DATA_SA_Crash.csv\"\n",
    "__UNIT_DATA__  = \"2018_DATA_SA_Units.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mock Classes that simulate a Pandas frame\n",
    "\n",
    "class loc_class:\n",
    "    def __init__(self):\n",
    "        self.rows = []\n",
    "        self.csv_df = None\n",
    "    \n",
    "    def add_row(self, item):\n",
    "        self.rows.append(item)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.rows[key]\n",
    "\n",
    "class csv_data_frame:\n",
    "    def __init__(self):\n",
    "        self.size = 0\n",
    "        self.header = {}\n",
    "        self.reverse_key = {}\n",
    "\n",
    "    def generate_row_alias(self, row):\n",
    "        insert_key = {}\n",
    "        for key in self.reverse_key.keys():\n",
    "            insert_key[self.reverse_key[key]] = row[key]\n",
    "        return insert_key\n",
    "        \n",
    "    def read_csv(self,filename):\n",
    "        self.loc = loc_class()\n",
    "        with open(filename,'rt')as f:\n",
    "          data = csv.reader(f)\n",
    "          row_val = 0\n",
    "          for row in data:\n",
    "            if row_val == 0 :\n",
    "                for i in range(len(row)):\n",
    "                    self.header[row[i]] = []\n",
    "                    self.reverse_key[i] = row[i]\n",
    "            else:\n",
    "                for i in range(len(row)):\n",
    "                    header_key = self.reverse_key[i]\n",
    "                    self.header[header_key].append(row[i])\n",
    "                self.loc.add_row(self.generate_row_alias(row))\n",
    "            row_val = row_val + 1\n",
    "            self.size = row_val\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.size-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(data):\n",
    "    for lin in data:\n",
    "        print(\"+---\"*len(lin)+\"+\")\n",
    "        for inlin in lin:\n",
    "            print(\"|\",str(inlin),\"\", end=\"\")\n",
    "        print(\"|\")\n",
    "    print(\"+---\"*len(lin)+\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "#crash_data_df = pd.read_csv(__CRASH_DATA__)\n",
    "#unit_data_df = pd.read_csv(__UNIT_DATA__)\n",
    "crash_data_df = csv_data_frame()\n",
    "crash_data_df.read_csv(__CRASH_DATA__)\n",
    "unit_data_df = csv_data_frame()\n",
    "unit_data_df.read_csv(__UNIT_DATA__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions to do things\n",
    "\n",
    "#Check if a value is an integer used to remove values which cause havoc with some \n",
    "#of the other functions\n",
    "def is_int(in_val):\n",
    "    try:\n",
    "        x = int(in_val)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "#Buffer Logic Class - Makes the code cleaner when dealing with Merging\n",
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.exhausted = False\n",
    "        self.size = 0\n",
    "        self.location = 0\n",
    "\n",
    "    def peek(self):\n",
    "        return self.queue[self.location]\n",
    "\n",
    "    def take(self):\n",
    "        self.location = self.location + 1\n",
    "\n",
    "    def is_exhausted(self):\n",
    "        if self.location >= self.size:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "#Manages a set of return buffers\n",
    "class BufferManager:\n",
    "    def __init__(self):\n",
    "        self.num_of_buffers = 0\n",
    "        self.buffers = []\n",
    "\n",
    "    #Returns the value in the buffers that is min\n",
    "    #Allows a cleaner implementation of merge\n",
    "    def get_min(self):\n",
    "        min_buff = None\n",
    "        for buff in self.buffers:\n",
    "            if buff.is_exhausted() == False:\n",
    "                if min_buff == None:\n",
    "                    min_buff = buff\n",
    "                if buff.peek()[1] < min_buff.peek()[1]:\n",
    "                    min_buff = buff\n",
    "        ret_val = min_buff.peek()\n",
    "        min_buff.take()\n",
    "        return ret_val\n",
    "    \n",
    "    def is_exhausted(self):\n",
    "        ret_val = True\n",
    "        for buff in self.buffers:\n",
    "            ret_val = ret_val and buff.is_exhausted()\n",
    "        return ret_val\n",
    "    \n",
    "    def add_buffer(self, temp_buff):\n",
    "        buff = Buffer()\n",
    "        buff.size = len(temp_buff)\n",
    "        buff.queue = temp_buff\n",
    "        self.buffers.append(buff)\n",
    "        self.num_of_buffers = self.num_of_buffers + 1\n",
    "        \n",
    "#A function that was identified to be common on all group by functions        \n",
    "def collate_table(hash_to_collate):\n",
    "    return_group_aggregate = {}\n",
    "    for key in hash_to_collate.keys():\n",
    "        return_group_aggregate[key] = sum(hash_to_collate[key])\n",
    "    return return_group_aggregate \n",
    "\n",
    "\n",
    "#Used by the Heap-Sort algorithm - a lot of code to deal \n",
    "# with useless nan's and non-values which make this algo break\n",
    "def generate_heap(input_list, size_of_list, subtree_index):\n",
    "    largest = subtree_index\n",
    "    left = 2 * subtree_index + 1     \n",
    "    right = 2 * subtree_index + 2   \n",
    "    if is_int(input_list[subtree_index][1]):\n",
    "        subtree_index_val = int(input_list[subtree_index][1])\n",
    "    else:\n",
    "        subtree_index_val = 0\n",
    "    \n",
    "    largest_val = subtree_index_val\n",
    "\n",
    "    if left < size_of_list:\n",
    "        left_val = 0\n",
    "        if is_int(input_list[left][1]):\n",
    "            left_val = int(input_list[left][1])\n",
    "        \n",
    "        if subtree_index_val < left_val: \n",
    "            largest = left \n",
    "            largest_val = left_val\n",
    "\n",
    "    if right < size_of_list: \n",
    "        if is_int(input_list[right][1]):\n",
    "            right_val = int(input_list[right][1])\n",
    "        else:\n",
    "            right_val = 0\n",
    "\n",
    "        if largest_val < right_val: \n",
    "            largest = right \n",
    "    if largest != subtree_index: \n",
    "        input_list[subtree_index],input_list[largest] = input_list[largest],input_list[subtree_index]  \n",
    "        generate_heap(input_list, size_of_list, largest) \n",
    "\n",
    "#A HOT-Encoder to generate values when doing rangepartitions on STRING literals\n",
    "def single_char_encoder(input_string, num_of_workers):\n",
    "    ascii_start = 97\n",
    "    ascii_end = 122\n",
    "    char_val = ord(input_string.lower()[0])\n",
    "    if char_val >= ascii_start and char_val <= ascii_end:\n",
    "        partition_range = ((ascii_end - ascii_start) +1 ) / (num_of_workers)\n",
    "        return int((char_val-ascii_start) // partition_range)\n",
    "    return ord(input_string.lower()[0]) % num_of_workers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Display formatter turns the output of the tasks to a dataframe thats more friendly\n",
    "def display_result(result_list, read_df):\n",
    "    my_list = []\n",
    "    columns = ['Date of Crash', 'Suburb', 'Postcode', 'Number of Casualties']\n",
    "    \n",
    "    \n",
    "    for item in result_list:\n",
    "        if isinstance(item, tuple) :\n",
    "            row_num = item[0]\n",
    "        else:\n",
    "            row_num = item\n",
    "        date_of_crash = str(read_df.loc[row_num]['Year']) + \"-\" + read_df.loc[row_num]['Month'] + \"-\" + read_df.loc[row_num]['Day'] \n",
    "        suburb = read_df.loc[row_num]['Suburb']\n",
    "        postcode = read_df.loc[row_num]['Postcode']\n",
    "        cas = read_df.loc[row_num]['Total Cas']\n",
    "        my_list.append([date_of_crash,suburb,postcode, cas])\n",
    "    print_matrix(my_list)\n",
    "\n",
    "#Display formatter turns the output of the tasks to a dataframe thats more friendly\n",
    "def display_result_task_2(results, show_max=20):\n",
    "    my_list = []\n",
    "    cycle = 0\n",
    "    columns = ['Date of Crash', 'Time', 'Suburb', 'Gender', 'Age', 'Number of Casualties', 'License Type']\n",
    "    \n",
    "    for key in results[0].keys():\n",
    "        items = results[0][key]\n",
    "        doc = str(crash_data_df.loc[key]['Year']) + \"-\" + crash_data_df.loc[key]['Month'] + \"-\" + crash_data_df.loc[key]['Day']\n",
    "        time = crash_data_df.loc[key]['Time']\n",
    "        suburb = crash_data_df.loc[key]['Suburb']     \n",
    "        cas = crash_data_df.loc[key]['Total Cas']\n",
    "        for it in items:    \n",
    "            gender = unit_data_df.loc[it]['Sex']\n",
    "            age = unit_data_df.loc[it]['Age']\n",
    "            lic_type = unit_data_df.loc[it]['Licence Type']\n",
    "            my_list.append([doc,time,suburb,gender,age,cas,lic_type])\n",
    "            cycle = cycle + 1\n",
    "        if cycle >= show_max:\n",
    "            break\n",
    "    print_matrix(my_list)\n",
    "\n",
    "#Display the following aggregate into a Dataframe\n",
    "def display_aggregate(values, column_one_name, column_two_name, show_max=20):\n",
    "    curr=0\n",
    "    target = values[0]\n",
    "    for merge_list in values:\n",
    "        if curr != 0:\n",
    "            target.update(values[curr])\n",
    "        curr = curr + 1\n",
    "    columns = [column_one_name, column_two_name]\n",
    "    my_list = []\n",
    "    size = 0\n",
    "    for key, value in target.items():\n",
    "        my_list.append([key,value])\n",
    "        size = size + 1\n",
    "        if size > show_max:\n",
    "            break\n",
    "    print_matrix(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of code is the schedulers - this is what does the initial partitioning as it schedules what each worker in the Python pool does. \n",
    "\n",
    "The implementation is just a list of ID's that scheduled to an array, if additional copy data is required a tuple of \n",
    "(ID, VAL) - is implemented. These scheduler do not expect a join, that is they don't generate an INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commmon Function that initialises the worker mailboxes so that each have their ID list, \n",
    "#this is common to all schedulers so made an own function\n",
    "def init_mailbox(worker_pool_size = __N_WORKERS__):\n",
    "    #Worker process mail\n",
    "    mailbox = []\n",
    "    for i in range(worker_pool_size):\n",
    "        mailbox.append([]) \n",
    "    return mailbox\n",
    "\n",
    "#The round-robin (random-equal) partition/scheduler it has a few tweaks to make it work on the assignment\n",
    "#it has a copy function which takes a single value for optimisation when for example sorting single values\n",
    "#It also removes invalid functions if enabled\n",
    "#Inputs:\n",
    "# -- data_frame : The Data frame to get ID and generate the work items\n",
    "# -- copy_item : The Item to copy to the worker mailbox\n",
    "# -- drop_invalid: Drop items if they are not a valid or expected data format\n",
    "# -- worker_pool_size: The number of processors to be activated\n",
    "# -- drop_func: Function that determines data validity - drops work item if False\n",
    "#Return: a List of Location ID, split across a set of array equal to the worker_pool_size\n",
    "def round_robin(data_frame, copy_item=None, drop_invalid=True, worker_pool_size = __N_WORKERS__, drop_func=is_int):\n",
    "    mailbox = init_mailbox()\n",
    "    if copy_item == None:\n",
    "        for i in range(len(data_frame)):\n",
    "            processor = i % worker_pool_size\n",
    "            mailbox[processor].append(i)\n",
    "        return mailbox\n",
    "    else:\n",
    "        for i in range(len(data_frame)):\n",
    "            processor = i % worker_pool_size\n",
    "            if drop_invalid:\n",
    "                if drop_func(data_frame.loc[i][copy_item]):\n",
    "                    item_val = (i, int(data_frame.loc[i][copy_item]))\n",
    "                    mailbox[processor].append(item_val)\n",
    "            else:\n",
    "                item_val = (i, data_frame.loc[i][copy_item])\n",
    "                mailbox[processor].append(item_val)  \n",
    "        return mailbox\n",
    "\n",
    "#The range_partition_scheduler partition/scheduler it takes an encoder which returns the CPU that\n",
    "#a specific item should be sent to a CPU\n",
    "#Inputs:\n",
    "# -- data_frame : The Data frame to get ID and generate the work items\n",
    "# -- range_key : The value that determines the range to be scheduled on\n",
    "# -- range_encoder: A function that returns the CPU to be allocated to a specific work item\n",
    "# -- worker_pool_size: The number of processors to be activated\n",
    "#Return: a List of Location ID, split over the worker_pool_size\n",
    "def range_partition_scheduler(data_frame, range_key, \n",
    "                              range_encoder = single_char_encoder, worker_pool_size = __N_WORKERS__):\n",
    "    mailbox = init_mailbox(worker_pool_size)\n",
    "    for i in range(len(data_frame)):\n",
    "        range_value = data_frame.loc[i][range_key]\n",
    "        processor = single_char_encoder(range_value, worker_pool_size)\n",
    "        mailbox[processor].append(i)  \n",
    "    return mailbox\n",
    "\n",
    "#This is similar to the partition scheduler above, except in one aspect\n",
    "#it is used after a group-by function has executed, and thus does not\n",
    "#operate on the DF to generate ID, rather it acts on the data structure directly\n",
    "def range_partition_scheduler_post_group(grouped_proc_values, range_encoder = single_char_encoder, \n",
    "                                         worker_pool_size = __N_WORKERS__):\n",
    "    mailbox = init_mailbox(worker_pool_size)\n",
    "    for proc_group_val in grouped_proc_values:\n",
    "        for key in proc_group_val.keys():\n",
    "            processor = single_char_encoder(key, worker_pool_size)\n",
    "            mailbox[processor].append((key,proc_group_val[key]))\n",
    "    return mailbox\n",
    "\n",
    "#The hash_group_scheduler partition/scheduler assigns specific hashgroups to a processor, based on a\n",
    "#Grouping\n",
    "#The scheduler tries to make sure that values that are grouped are not some how lost in a dependency \n",
    "#loop.\n",
    "#Inputs:\n",
    "# -- data_frame : The Data frame to get ID and generate the work items\n",
    "# -- hash_key : The value that determines the hash group to be scheduled on\n",
    "# -- has_value: A function that returns the CPU to be allocated to a specific work item\n",
    "# -- value_key : The key to group on\n",
    "# -- drop_func: Function that determines data validity - drops work item if False\n",
    "# -- worker_pool_size: The number of processors to be activated\n",
    "#Return: a List of Location ID, split over the worker_pool_size\n",
    "def hash_group_scheduler(data_frame, hash_key, has_value=True, value_key=None, \n",
    "                         drop_func=is_int, worker_pool_size = __N_WORKERS__):\n",
    "    mailbox = init_mailbox(worker_pool_size)\n",
    "    counter = 0\n",
    "    cpu_index = {}\n",
    "    for i in range(len(data_frame)):\n",
    "        if has_value:\n",
    "            if drop_func(data_frame.loc[i][value_key]) == False:\n",
    "                continue\n",
    "            if data_frame.loc[i][hash_key] in cpu_index.keys():\n",
    "                mailbox[cpu_index[data_frame.loc[i][hash_key]]].append(i)\n",
    "            else:\n",
    "                cpu_index[data_frame.loc[i][hash_key]] = counter % worker_pool_size\n",
    "                counter = counter + 1\n",
    "        else:\n",
    "            if data_frame.loc[i][hash_key] in cpu_index.keys():\n",
    "                mailbox[cpu_index[data_frame.loc[i][hash_key]]].append(i)\n",
    "            else:\n",
    "                cpu_index[data_frame.loc[i][hash_key]] = counter % worker_pool_size\n",
    "                counter = counter + 1\n",
    "    return mailbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next of schedulers are the join schedulers, they work by generating one-time indecies on the values that are going to be joined, they return an ITEM_INDEX variable which is an index for the names and the mailbox which is the work to be done for each partition\n",
    "\n",
    "Each Scheduler also deals with data-dependencies, that is it makes sure that items are not going to be sent to \n",
    "disparate processors which cannot read the data.\n",
    "\n",
    "This works by assigning the parent table - the one that generates the index - to one processor. Then the inner table join attribute is scanned to see which index it points to, it then uses that index to get the CPU allocation, then it uses that to map that work item to that CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The broad_cast_join_scheduler assigns specific items to a processor, \n",
    "#The scheduler guarantees that each item in the inner table is located to its parent table\n",
    "#The scheduler is similar/based to round robin\n",
    "#Inputs:\n",
    "# -- data_frame : The Data frame to get ID and generate the work items\n",
    "# -- data_frame_dep : The data frame that depends (or inner table) on the parent data frame\n",
    "# -- index_name: the Attribute which will be used to join both tables\n",
    "# -- worker_pool_size: The number of processors to be activated\n",
    "#Return: a List of Location ID, split over the worker_pool_size + and Index\n",
    "def broad_cast_join_scheduler(data_frame_index, data_frame_dep, index_name,\n",
    "                              worker_pool_size = __N_WORKERS__):\n",
    "    mailbox = init_mailbox()\n",
    "    item_index = {}\n",
    "    cpu_index = {}\n",
    "    for i in range(len(data_frame_index)):\n",
    "        item_index[data_frame_index.loc[i][index_name]] = i\n",
    "        processor = i % worker_pool_size\n",
    "        cpu_index[data_frame_index.loc[i][index_name]] = processor\n",
    "\n",
    "    for i in range(len(data_frame_dep)):\n",
    "        mailbox[cpu_index[data_frame_dep.loc[i][index_name]]].append(i)\n",
    "        \n",
    "    return (item_index, mailbox)\n",
    "\n",
    "#The range_partition_join_scheduler assigns specific items to a processor, \n",
    "#The scheduler guarantees that each item in the inner table is located to its parent table\n",
    "#The scheduler is similar works out the item ranges and splits them over the processors\n",
    "#Inputs:\n",
    "# -- data_frame : The Data frame to get ID and generate the work items\n",
    "# -- data_frame_dep : The data frame that depends (or inner table) on the parent data frame\n",
    "# -- index_name: the Attribute which will be used to join both tables\n",
    "# -- worker_pool_size: The number of processors to be activated\n",
    "#Return: a List of Location ID, split over the worker_pool_size + and Index\n",
    "def range_partition_join_scheduler(data_frame_index, data_frame_dep, index_name, \n",
    "                                   worker_pool_size = __N_WORKERS__):\n",
    "    mailbox = init_mailbox()\n",
    "    item_index = {}\n",
    "    cpu_index = {}\n",
    "    partition_range = (len(data_frame_index) +1 ) / (worker_pool_size)\n",
    "    for i in range(len(data_frame_index)):\n",
    "        item_index[data_frame_index.loc[i][index_name]] = i\n",
    "        processor = int(i // partition_range)\n",
    "        cpu_index[data_frame_index.loc[i][index_name]] = processor\n",
    "\n",
    "    for i in range(len(data_frame_dep)):\n",
    "        mailbox[cpu_index[data_frame_dep.loc[i][index_name]]].append(i)\n",
    "        \n",
    "    return (item_index, mailbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Searches \n",
    "\n",
    "The Search is done through two functions which are quite large as the deal with a whole range of conditions and\n",
    "multiple data types:\n",
    "\n",
    "Linear-Filter is \"Linear Search\" I called it filter since it just does that removes stuff that is not the value\n",
    "specified. However It has a remove field - if enabled it lets everything else pass EXCEPT the value specified.\n",
    "\n",
    "Binary Search is very similar to a normal binary search except it has the range specifiers which allow one\n",
    "to use it for range searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search Linearly for values, and if the value passes move the ID forward\n",
    "#Inputs:\n",
    "# -- search_key : The column searching\n",
    "# -- search_item : The value to match\n",
    "# -- worker_mailbox: The input list of ID's to match\n",
    "# -- data_frame: The data frame that the ID's reference to\n",
    "# -- remove: Rather than pass the value - Remove ID that match the value\n",
    "# -- alternate_field: Look in the JOINED innertable rather than the OUTER\n",
    "#Return: a List of Location ID, that match the specified criteria\n",
    "\n",
    "def linear_filter(search_key, search_item, worker_mailbox, data_frame, remove=False, alternate_field=False):\n",
    "    filter_list = []\n",
    "    if isinstance(worker_mailbox, dict):\n",
    "        ret_dict = {}\n",
    "        for key in worker_mailbox:\n",
    "            if alternate_field:\n",
    "                ret_box = []\n",
    "                for inner_table_loc in worker_mailbox[key]:\n",
    "                    inner_val = data_frame.loc[inner_table_loc]\n",
    "                    if inner_val[search_key] == search_item:\n",
    "                        ret_box.append(inner_table_loc)\n",
    "                if len(ret_box) >=1 :\n",
    "                    ret_dict[key] = ret_box\n",
    "            else:\n",
    "                row_val = data_frame.loc[key]\n",
    "                if row_val[search_key] == search_item:\n",
    "                    ret_dict[key] = worker_mailbox[key]\n",
    "        return ret_dict\n",
    "    for item in worker_mailbox:\n",
    "        if isinstance(item, tuple) :\n",
    "            row_num = item[0]\n",
    "        else:\n",
    "            row_num = item\n",
    "        row_val = data_frame.loc[row_num]\n",
    "        if remove == False:\n",
    "            if row_val[search_key] == search_item:\n",
    "                filter_list.append(item)\n",
    "        else:\n",
    "            if row_val[search_key] != str(search_item):\n",
    "                filter_list.append(item)\n",
    "    return filter_list\n",
    "\n",
    "#Use a binary search, and if the value passes move the ID forward\n",
    "#Inputs:\n",
    "# -- search_key : The column searching\n",
    "# -- search_item : The value to match\n",
    "# -- worker_mailbox: The input list of ID's to match\n",
    "# -- data_frame: The data frame that the ID's reference to\n",
    "# -- range_search: Search for a RANGE - MIN OR MAX\n",
    "# -- range_greater: If True its a Greater-Equal Search\n",
    "#Return: a List of Location ID, that match the specified criteria\n",
    "def binary_search(search_key, search_item, worker_mailbox, data_frame, range_search=False, range_greater=True):\n",
    "    worker_mailbox.sort(key = operator.itemgetter(1)) #Sort List\n",
    "    partition_start = 0\n",
    "    partition_end = len(worker_mailbox) - 1\n",
    "    return_list = []\n",
    "    while(partition_start <= partition_end):\n",
    "        mid = (partition_start + partition_end)//2\n",
    "        if int(worker_mailbox[mid][1]) == search_item :\n",
    "            if range_search:\n",
    "                if range_greater:\n",
    "                    return worker_mailbox[mid:]\n",
    "                else:\n",
    "                    return worker_mailbox[:mid]\n",
    "            else:\n",
    "                return_list.append(worker_mailbox[mid][0])\n",
    "                #Move up one, if this is the only item it would terminate\n",
    "                partition_start = mid + 1\n",
    "        else:\n",
    "            if search_item < int(worker_mailbox[mid][1]):\n",
    "                partition_end = mid - 1\n",
    "            else:\n",
    "                partition_start = mid + 1\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Functions\n",
    "\n",
    "This includes the other operations/functions required to complete the assingment such as merge, sorting, joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function depends on the JOIN SChedulers to setup the appropriate data structure\n",
    "#It is based on the index, and does a hash-based join, it places the items that \n",
    "#match the join criteria in to the work mail box so the format is\n",
    "# PARENT_ID: [CHILD1, CHILD2]\n",
    "#Inputs:\n",
    "# -- worker_mailbox: The input list\n",
    "# -- df_ix: The Parent (INDEX) data frame\n",
    "# -- df_dep: The child, dependent list, whose values will be embededd\n",
    "# -- index: the index tables of the parent\n",
    "# -- index_name: the name of the attribute used to match the CHILD table\n",
    "def parallel_join(worker_mailbox, df_ix, df_dep, index, index_name):\n",
    "    joined_list = {}\n",
    "    for row_num in worker_mailbox:\n",
    "        item  = df_dep.loc[row_num]\n",
    "        idx_num = index[item[index_name]]\n",
    "        if idx_num in joined_list:\n",
    "            joined_list[idx_num].append(row_num)\n",
    "        else:\n",
    "            joined_list[idx_num] = [row_num]\n",
    "    return joined_list\n",
    "\n",
    "#Heap Sort - This function also depends on the scheduler doing its thing right to setup the datastructure\n",
    "# so it can sort it depends on the input list being in [(ID,VAL), (ID,VAL)] - will sort on VAL\n",
    "#Inputs:\n",
    "# -- input_list: The input list to sort\n",
    "# -- data_frame: The data frame\n",
    "\n",
    "def heap_sort(input_list, data_frame):  \n",
    "    size = len(input_list) \n",
    "    for i in range(size-1, -1, -1): \n",
    "        generate_heap(input_list, size, i) \n",
    "    for i in range(size-1, -1, -1): \n",
    "        input_list[i], input_list[0] = input_list[0], input_list[i] \n",
    "        generate_heap(input_list, i, 0) \n",
    "    return input_list\n",
    "\n",
    "#Merges the results returned by individial partitions\n",
    "#Inputs:\n",
    "# -- input queues : the returned queues after sorting that need to be merged\n",
    "def merge(input_queues):\n",
    "    buffer_manager = BufferManager()\n",
    "    sorted_list = []\n",
    "    for item in input_queues:\n",
    "        buffer_manager.add_buffer(item)\n",
    "    while(buffer_manager.is_exhausted() == False):\n",
    "        sorted_list.append(buffer_manager.get_min())\n",
    "    return sorted_list\n",
    "\n",
    "#This aggregates the - GROUPS - a set of data based on a key - It then executes the sum\n",
    "#Inputs:\n",
    "# -- worker_mailbox: The input list\n",
    "# -- data_frame: The data frame that the ID's reference to\n",
    "# -- group_attribute:  How to Group the items\n",
    "# -- sum_attribute : What to add\n",
    "def aggregate_item_by_group(worker_mailbox, data_frame, group_attribute, sum_attribute):\n",
    "    hash_group = {}\n",
    "    for row_num in worker_mailbox:\n",
    "        if data_frame.loc[row_num][group_attribute] not in hash_group.keys():\n",
    "            hash_group[data_frame.loc[row_num][group_attribute]] = []\n",
    "        hash_group[data_frame.loc[row_num][group_attribute]].append(int(data_frame.loc[row_num][sum_attribute]))\n",
    "    return collate_table(hash_group)\n",
    "\n",
    "#This aggregates the - GROUPS - However it assumes another GROUP has happened before and thus does not need\n",
    "#to refer to the original data structure\n",
    "#Inputs:\n",
    "# -- worker_mailbox: The input list \n",
    "def aggregate_grouped_values(worker_mailbox):\n",
    "    hash_group = {}\n",
    "    for item in worker_mailbox:\n",
    "        if item[0] not in hash_group.keys():\n",
    "            hash_group[item[0]] = []\n",
    "        hash_group[item[0]].append(int(item[1]))\n",
    "    return collate_table(hash_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "\n",
    "Now this is just the code - I'll be walking through it however I won't run them UNTIL the end once the __MAIN__ macro has passed to demonstrate that the code works correctly - This is because the PYTHON script wont have the correct pool until that MACRO is run the the workers are set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 1 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Wednesday | ADELAIDE | 5000 | 1 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Saturday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Saturday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Sunday | ADELAIDE | 5000 | 4 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | ADELAIDE | 5000 | 1 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Saturday | ADELAIDE | 5000 | 1 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | ADELAIDE | 5000 | 2 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Saturday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Wednesday | ADELAIDE | 5000 | 1 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Monday | ADELAIDE | 5000 | 2 |\n",
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | ADELAIDE | 5000 | 0 |\n",
      "+---+---+---+---+\n",
      "CPU times: user 655 ms, sys: 108 ms, total: 762 ms\n",
      "Wall time: 970 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#The task Kernel using two linear_filters to filter the data out\n",
    "#returns the list of filtered items (ID)\n",
    "def task_1_1_kernel(worker_mailbox, data_frame):\n",
    "    suburb_filtered = linear_filter('Suburb','ADELAIDE', worker_mailbox, data_frame)\n",
    "    month_filtered = linear_filter('Month','January',suburb_filtered,data_frame)\n",
    "    return month_filtered\n",
    "\n",
    "def task_1_1():\n",
    "    mailbox = round_robin(crash_data_df)\n",
    "    #Launch kernels\n",
    "    kernel_results = [pool.apply_async(task_1_1_kernel, (mailbox[i], crash_data_df)) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get() for res in kernel_results]\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    merged =  flatten(values)[:20]\n",
    "    return merged\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "\n",
    "%time display_result(task_1_1(), crash_data_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.2\n",
    "\n",
    "For this task I chose ROUND-ROBIN, It is a search for a specific item, and if I wanted to fill up the\n",
    "CPU tasks, I want to do so evenly as possible to achieve possible maximum throughput \n",
    "Only round-robin can possibly guarantee an even distribution of tasks across all processors and hence\n",
    "why I chose it. \n",
    "The task is not dependent on any other value than it self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| 2018-January-Tuesday | MOUNT SCHANK | 5291 | 7 |\n",
      "+---+---+---+---+\n",
      "| 2018-July-Tuesday | CRAIGMORE | 5114 | 7 |\n",
      "+---+---+---+---+\n",
      "CPU times: user 665 ms, sys: 113 ms, total: 777 ms\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "#Kernel just executes the binary search \n",
    "def task_1_2_kernel(worker_mailbox, data_frame):\n",
    "    return binary_search('Total Cas', 7, worker_mailbox, data_frame)\n",
    "\n",
    "\n",
    "def task_1_2():\n",
    "    mailbox = round_robin(crash_data_df, 'Total Cas')\n",
    "    kernel_results = [pool.apply_async(task_1_2_kernel, (mailbox[i], crash_data_df)) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get() for res in kernel_results]\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    merged =  flatten(values)\n",
    "    return merged\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "\n",
    "%time display_result(task_1_2(), crash_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.3\n",
    "\n",
    "For this task I chose ROUND-ROBIN, It is a search for a specific item, and if I wanted to fill up the\n",
    "CPU tasks, I want to do so evenly as possible to achieve possible maximum throughput \n",
    "Only round-robin can possibly guarantee an even distribution of tasks across all processors and hence\n",
    "why I chose it. \n",
    "The task is not dependent on any other value than it self.\n",
    "\n",
    "For the Greater search I used the binary filter as the implementation was straight forward to return the python slice to left or right of the value. No need to use the linear search as only the pivot point is required in the binary version and this is much faster\n",
    "\n",
    "For the suburb search I still used  a linear filter to remove suburbs other than adelaide, this avoid complications with trying to measure the \"index\" of a word - its safer to do so on an unknown type\n",
    "\n",
    "Lastly in the kernel I added an additional filter to remove the value 3, this is because whats asked is for >3 not >=3, the binary search does a >=3.. this removes any additional stray data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| 2018-January-Sunday | ADELAIDE | 5000 | 4 |\n",
      "+---+---+---+---+\n",
      "CPU times: user 706 ms, sys: 125 ms, total: 831 ms\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "def task_1_3_kernel(worker_mailbox, data_frame):\n",
    "    suburb_filtered = linear_filter('Suburb','ADELAIDE', worker_mailbox, data_frame)\n",
    "    great_search =  binary_search('Total Cas', 3, suburb_filtered, data_frame, True, True)\n",
    "    return linear_filter('Total Cas',3, great_search, data_frame, True)\n",
    "\n",
    "def task_1_3():\n",
    "    mailbox = round_robin(crash_data_df, 'Total Cas')\n",
    "    kernel_results = [pool.apply_async(task_1_3_kernel, (mailbox[i], crash_data_df)) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get() for res in kernel_results]\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    merged =  flatten(values)\n",
    "    return merged\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time display_result(task_1_3(), crash_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2.1\n",
    "\n",
    "The _broad_cast_join_scheduler_ function is based on the round-robin scheduler, with additional modifications to make it a divide and broadcast algorith. It also has additional key feature that round-robin doesnt. Round-Robin assumes no dependencies between the tasks, this one does. This and _range_partition_join_scheduler_ __ASSUME__ that a Hash Join is going to be used as implemented by the _parallel_join_ function. \n",
    "\n",
    "The choice was made in due to data-dependency resolution, I wanted to avoid sharing data that requires CPU cross boundry communication since _PYTHON's_ concurrency is quite poor. \n",
    "\n",
    "No _kernel task_ function exists since the JOIN is the final operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| 2018-October-Wednesday | 11:20 am | MITCHELL PARK | Male | 018 | 0 | Provisional 1  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-October-Wednesday | 11:20 am | MITCHELL PARK | Male | XXX | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-October-Wednesday | 11:20 am | MITCHELL PARK | Female | 066 | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-October-Wednesday | 11:20 am | MITCHELL PARK |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:00 am | CROYDON | Female | 030 | 3 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:00 am | CROYDON | Male | 032 | 3 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 10:00 am | KENSINGTON | Female | 058 | 0 | Provisional 2 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 10:00 am | KENSINGTON | Female | 022 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 10:00 am | KENSINGTON | Male | 073 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 02:30 pm | ONE TREE HILL | Male | 040 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 02:30 pm | ONE TREE HILL |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 02:30 pm | ONE TREE HILL |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 03:20 pm | FOREST RANGE | Male | 021 | 1 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 03:20 pm | FOREST RANGE |  |  | 1 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:20 pm | WESTBOURNE PARK | Female | 043 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:20 pm | WESTBOURNE PARK | Male | 039 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:20 pm | WESTBOURNE PARK |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 11:58 am | WOODVILLE WEST |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 11:58 am | WOODVILLE WEST | Male | 025 | 0 | Provisional 2 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 11:58 am | WOODVILLE WEST |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "CPU times: user 1.41 s, sys: 245 ms, total: 1.65 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "def task_2_1():\n",
    "    schedule = broad_cast_join_scheduler(crash_data_df, unit_data_df, 'REPORT_ID')\n",
    "    kernel_results = [pool.apply_async(parallel_join, (schedule[1][i],crash_data_df, unit_data_df, schedule[0], 'REPORT_ID')) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    return values\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time display_result_task_2(task_2_1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2.2\n",
    "\n",
    "The _range_partition_join_scheduler_ tries to partition across the range, it was discovered that because of the indexing and hashing properties required for the join to work, the unique_id field format wasn't an issue. It was already in line with what PANDAS was reporting as its index, and when used as a HASH the format wasn't an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:15 am | ADELAIDE | Male | 049 | 2 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:15 am | ADELAIDE | Male | 032 | 2 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:10 pm | ADELAIDE | Female | 056 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:10 pm | ADELAIDE | Female | 036 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 04:10 pm | ADELAIDE |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Tuesday | 11:18 am | ADELAIDE | Male | 087 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Tuesday | 11:18 am | ADELAIDE |  |  | 0 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Tuesday | 05:10 pm | ADELAIDE | Female | 037 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Tuesday | 05:10 pm | ADELAIDE | Unknown | XXX | 0 | Unknown |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Wednesday | 06:15 pm | ADELAIDE | Male | 059 | 1 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:35 pm | ADELAIDE | Female | 023 | 1 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:35 pm | ADELAIDE | Male | 043 | 1 |  |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:30 pm | ADELAIDE | Female | 025 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Monday | 05:30 pm | ADELAIDE | Unknown | XXX | 0 | Unknown |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Tuesday | 06:47 pm | ADELAIDE | Male | 043 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Tuesday | 06:47 pm | ADELAIDE | Female | 019 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Wednesday | 02:05 pm | ADELAIDE | Male | 040 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Wednesday | 02:05 pm | ADELAIDE | Female | 022 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Wednesday | 05:40 pm | ADELAIDE | Female | 064 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2018-January-Wednesday | 05:40 pm | ADELAIDE | Female | 026 | 0 | Full |\n",
      "+---+---+---+---+---+---+---+\n",
      "CPU times: user 1.47 s, sys: 220 ms, total: 1.69 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "def task_2_2_kernel(worker_mailbox, df_ix, df_dep, index, index_name):\n",
    "    join_list = parallel_join(worker_mailbox, df_ix, df_dep, index, index_name)\n",
    "    return linear_filter('Suburb','ADELAIDE', join_list, df_ix)\n",
    "\n",
    "def task_2_2():\n",
    "    schedule = range_partition_join_scheduler(crash_data_df, unit_data_df, 'REPORT_ID')\n",
    "    kernel_results = [pool.apply_async(task_2_2_kernel, (schedule[1][i],crash_data_df, unit_data_df, schedule[0], 'REPORT_ID')) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    return values\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time display_result_task_2(task_2_2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3.1\n",
    "\n",
    "I used round-robin for mostly performance reasons, I wanted to make sure all the processors had their worker queues full with tasks. As the input did not depend on any other external factors. A range scheduler could of worked well here as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| 2018-May-Saturday | ROSTREVOR | 5073 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-August-Wednesday | KILKENNY | 5009 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-October-Monday | ST GEORGES | 5064 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-September-Saturday | RIDGEHAVEN | 5097 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-August-Saturday | BIRKENHEAD | 5015 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-August-Wednesday | WINGFIELD | 5013 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-November-Saturday | TORRENSVILLE | 5031 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-October-Monday | WINGFIELD | 5013 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-October-Sunday | COWIRRA | 5238 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-May-Saturday | UPPER STURT | 5156 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-May-Saturday | EDEN HILLS | 5050 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-May-Tuesday | BROOKLYN PARK | 5032 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-May-Friday | MANNINGHAM | 5086 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-August-Wednesday | MODBURY HEIGHTS | 5092 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-November-Monday | NACKARA | 5440 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-November-Saturday | PORT LINCOLN | 5606 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-June-Thursday | NORWOOD | 5067 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-October-Tuesday | PLYMPTON | 5038 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-December-Saturday | SEAFORD | 5169 | 0 |\n",
      "+---+---+---+---+\n",
      "| 2018-October-Sunday | NURIOOTPA | 5355 | 0 |\n",
      "+---+---+---+---+\n",
      "CPU times: user 683 ms, sys: 111 ms, total: 794 ms\n",
      "Wall time: 997 ms\n"
     ]
    }
   ],
   "source": [
    "def task_3_1():\n",
    "    mailbox = round_robin(crash_data_df, 'Total Cas')\n",
    "    kernel_results = [pool.apply_async(heap_sort, (mailbox[i], crash_data_df)) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    return display_result(merge(values)[:20], crash_data_df)\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time task_3_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3.2\n",
    "\n",
    "Like in 3.1 I went with round robin to make full use of the available CPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| 2018-9371-15/08/2019 | 01 | SA | 1900 | UNKNOWN |\n",
      "+---+---+---+---+---+\n",
      "| 2018-12792-15/08/2019 | 01 | SA | 1900 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-6385-15/08/2019 | 01 | SA | 1900 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-12484-15/08/2019 | 01 | SA | 1934 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-2351-15/08/2019 | 01 | SA | 1951 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-2981-15/08/2019 | 01 | SA | 1955 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-238-15/08/2019 | 01 | SA | 1957 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-1583-15/08/2019 | 01 | SA | 1957 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-7265-15/08/2019 | 02 | VIC | 1962 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-4197-15/08/2019 | 01 | SA | 1964 | UNKNOWN |\n",
      "+---+---+---+---+---+\n",
      "| 2018-11816-15/08/2019 | 01 | VIC | 1965 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-12413-15/08/2019 | 02 | SA | 1965 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-6392-15/08/2019 | 01 | SA | 1965 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-2281-15/08/2019 | 02 | SA | 1966 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-12810-15/08/2019 | 02 | SA | 1967 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-5065-15/08/2019 | 01 | SA | 1967 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-3626-15/08/2019 | 01 | SA | 1968 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-7498-15/08/2019 | 01 | SA | 1969 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-2893-15/08/2019 | 02 | SA | 1969 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-2652-15/08/2019 | 01 | SA | 1969 | SA |\n",
      "+---+---+---+---+---+\n",
      "| 2018-11274-15/08/2019 | 02 | SA | 1969 | SA |\n",
      "+---+---+---+---+---+\n",
      "CPU times: user 794 ms, sys: 132 ms, total: 926 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "def task_3_2():\n",
    "    mailbox = round_robin(unit_data_df, 'Veh Year', True)\n",
    "    kernel_results = [pool.apply_async(heap_sort, (mailbox[i], unit_data_df)) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    #Here is the big difference, we now need to merge these in a pipeline\n",
    "    #There needs to be a schedule that distributes the load across all workers\n",
    "    #We are also going to reduce the amount of items so it should be really N/2\n",
    "    n_pools = int(len(values) // 2)\n",
    "    worker_box = []\n",
    "    while n_pools >= 2 :\n",
    "        worker_box = []\n",
    "        for i in range(n_pools):\n",
    "            worker_box.append([]) \n",
    "        for i in range(len(values)):\n",
    "            processor = int(i // n_pools)\n",
    "            worker_box[processor].append(values[i])\n",
    "        parallel_merge_results = [pool.apply_async(merge, ([worker_box[i]])) for i in range(n_pools)]\n",
    "        values = [res.get(timeout=10) for res in parallel_merge_results]\n",
    "        n_pools = int(len(values) // 2)\n",
    "    disp_val =  merge(values)\n",
    "    my_list = []\n",
    "    columns = ['Report Id', 'Unit No', 'Vehicle Reg State', 'Vehicle Year', 'State License']\n",
    "    for item in disp_val:\n",
    "        if isinstance(item, tuple) :\n",
    "            row_num = item[0]\n",
    "        else:\n",
    "            row_num = item\n",
    "        rep_id = str(unit_data_df.loc[row_num]['REPORT_ID'])  \n",
    "        unit_no = unit_data_df.loc[row_num]['Unit No']\n",
    "        reg = unit_data_df.loc[row_num]['Veh Reg State']\n",
    "        year = unit_data_df.loc[row_num]['Veh Year']\n",
    "        lic = unit_data_df.loc[row_num]['Lic State']\n",
    "        my_list.append([rep_id,unit_no, reg, year, lic])\n",
    "        if len(my_list) > 20:\n",
    "            break\n",
    "    print_matrix(my_list)\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time task_3_2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate() ## Terminate the pool to prevent leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4.1\n",
    "\n",
    "For this task I implemented a psuedo hash-scheduler in _hash_group_scheduler_ psuedo because it uses Python dictionaries and not a hash-function, and more importantly its a group based scheduler. It makes an explicit assumption that the groups are going to be the main unit of scheduling. This also means that the CPU load is also unbalanced, as the data might not follow the group. The groups might be scheduled evenly but the tasks might not be so we'd endup with skewed worker queues.\n",
    "\n",
    "The reason for use was very simple, even thought it has a skewed distribution it also prevents overlap of tasks. Dependencies are located to one processor. Which made the implementation much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| Right Angle | 1266 |\n",
      "+---+---+\n",
      "| Hit Pedestrian | 297 |\n",
      "+---+---+\n",
      "| Other | 26 |\n",
      "+---+---+\n",
      "| Hit Object on Road | 8 |\n",
      "+---+---+\n",
      "| Rear End | 1680 |\n",
      "+---+---+\n",
      "| Hit Animal | 44 |\n",
      "+---+---+\n",
      "| Side Swipe | 387 |\n",
      "+---+---+\n",
      "| Right Turn | 604 |\n",
      "+---+---+\n",
      "| Hit Parked Vehicle | 224 |\n",
      "+---+---+\n",
      "| Left Road - Out of Control | 52 |\n",
      "+---+---+\n",
      "| Hit Fixed Object | 846 |\n",
      "+---+---+\n",
      "| Roll Over | 449 |\n",
      "+---+---+\n",
      "| Head On | 234 |\n",
      "+---+---+\n",
      "CPU times: user 678 ms, sys: 105 ms, total: 783 ms\n",
      "Wall time: 974 ms\n"
     ]
    }
   ],
   "source": [
    "def task_4_1():\n",
    "    mailbox = hash_group_scheduler(crash_data_df, 'Crash Type', has_value=True, value_key='Total Cas')\n",
    "    kernel_results = [pool.apply_async(aggregate_item_by_group, (mailbox[i], crash_data_df, 'Crash Type', 'Total Cas')) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    return display_aggregate(values,'Crash Type', 'Number')\n",
    "    \n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "\n",
    "%time task_4_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate() ## Terminate the pool to prevent leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4.2\n",
    "\n",
    "There are two key points as to why this can be better:\n",
    "\n",
    "1. Round Robin - Makes no dependencies assumptions, this is because the algorithm doesn't require it, where as the _hash_group_scheduler_ tries to avoid making these depency assumptions, the two phase method doesn't even bother, making its implementation much simpler, easier and possibly can be applied to more scenarios than the one in 4.1\n",
    "2. Performance 12s vs 3.1 seconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| CROYDON | 12 |\n",
      "+---+---+\n",
      "| FOREST RANGE | 3 |\n",
      "+---+---+\n",
      "| BOLIVAR | 9 |\n",
      "+---+---+\n",
      "| CURRENCY CREEK | 10 |\n",
      "+---+---+\n",
      "| GILLES PLAINS | 11 |\n",
      "+---+---+\n",
      "| ELIZABETH EAST | 25 |\n",
      "+---+---+\n",
      "| ALDINGA | 10 |\n",
      "+---+---+\n",
      "| FULLARTON | 11 |\n",
      "+---+---+\n",
      "| BRIGHTON | 19 |\n",
      "+---+---+\n",
      "| GLENUNGA | 5 |\n",
      "+---+---+\n",
      "| ELIZABETH DOWNS | 17 |\n",
      "+---+---+\n",
      "| ADELAIDE | 211 |\n",
      "+---+---+\n",
      "| BEDFORD PARK | 19 |\n",
      "+---+---+\n",
      "| CRAIGMORE | 18 |\n",
      "+---+---+\n",
      "| ELIZABETH | 20 |\n",
      "+---+---+\n",
      "| ELIZABETH PARK | 26 |\n",
      "+---+---+\n",
      "| BROADVIEW | 15 |\n",
      "+---+---+\n",
      "| FINDON | 19 |\n",
      "+---+---+\n",
      "| ENFIELD | 33 |\n",
      "+---+---+\n",
      "| GEPPS CROSS | 46 |\n",
      "+---+---+\n",
      "| BURRA | 3 |\n",
      "+---+---+\n",
      "CPU times: user 634 ms, sys: 113 ms, total: 747 ms\n",
      "Wall time: 935 ms\n"
     ]
    }
   ],
   "source": [
    "def task_4_2():\n",
    "    mailbox = round_robin(crash_data_df)\n",
    "    kernel_results = [pool.apply_async(aggregate_item_by_group, (mailbox[i], crash_data_df, 'Suburb', 'Total Cas')) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    mailbox = range_partition_scheduler_post_group(values)\n",
    "    kernel_results = [pool.apply_async(aggregate_grouped_values, ([mailbox[i]])) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    return display_aggregate(values,'Suburb', 'Number')\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time task_4_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate() ## Terminate the pool to prevent leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 5\n",
    "\n",
    "For this task, I wanted to use the Two-Phase method for the parallel group by, however that meant I was locked to use the broad_cast_join_scheduler I already have. It does a lot of good dependency tracking when doing the join. The Join has to come first since the Group attribute is not the join attribute. \n",
    "For the license location I place the linear-filter in the next step of the pipeline, I had done this because by this stage the attributes have been joined and it makes the filtering easier and predictable (all the ones not matching the criteria are all removed)\n",
    "\n",
    "Then there are two aggreate functions, the first run in the kernel task to group by the suburb and total casualties, and the similarly as the Two-Phase method another aggregate is called to do the global aggregation \n",
    "\n",
    "I wanted to use Two-Phase due to its impressive performance, given that this task includes all the above operations already done, and others seemed significantly slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| GILLES PLAINS | 4 |\n",
      "+---+---+\n",
      "| ANDREWS FARM | 3 |\n",
      "+---+---+\n",
      "| BLAIR ATHOL | 1 |\n",
      "+---+---+\n",
      "| ETHELTON | 0 |\n",
      "+---+---+\n",
      "| ELIZABETH DOWNS | 4 |\n",
      "+---+---+\n",
      "| ELIZABETH SOUTH | 0 |\n",
      "+---+---+\n",
      "| FULHAM GARDENS | 0 |\n",
      "+---+---+\n",
      "| GAWLER | 0 |\n",
      "+---+---+\n",
      "| ELIZABETH EAST | 3 |\n",
      "+---+---+\n",
      "| DAVOREN PARK | 2 |\n",
      "+---+---+\n",
      "| BLAKEVIEW | 2 |\n",
      "+---+---+\n",
      "| GLENELG | 0 |\n",
      "+---+---+\n",
      "| ADELAIDE | 5 |\n",
      "+---+---+\n",
      "| ASCOT PARK | 1 |\n",
      "+---+---+\n",
      "| EVANSTON PARK | 1 |\n",
      "+---+---+\n",
      "| CAMPBELLTOWN | 0 |\n",
      "+---+---+\n",
      "| ELIZABETH PARK | 5 |\n",
      "+---+---+\n",
      "| FLINDERS PARK | 2 |\n",
      "+---+---+\n",
      "| GEPPS CROSS | 0 |\n",
      "+---+---+\n",
      "| GAWLER WEST | 3 |\n",
      "+---+---+\n",
      "| CRAIGMORE | 0 |\n",
      "+---+---+\n",
      "CPU times: user 1.44 s, sys: 221 ms, total: 1.66 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "def task_5_1_kernel(worker_mailbox, df_ix, df_dep, index, index_name):\n",
    "    join_list = parallel_join(worker_mailbox, df_ix, df_dep, index, index_name)\n",
    "    filtered_list =  linear_filter('Licence Type','Unlicenced', join_list, df_dep, False, True)\n",
    "    return aggregate_item_by_group(filtered_list.keys(), df_ix, 'Suburb', 'Total Cas')\n",
    "\n",
    "def task_5():\n",
    "    schedule = broad_cast_join_scheduler(crash_data_df, unit_data_df, 'REPORT_ID')\n",
    "    kernel_results = [pool.apply_async(task_5_1_kernel, (schedule[1][i],crash_data_df, unit_data_df, schedule[0], 'REPORT_ID')) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    mailbox = range_partition_scheduler_post_group(values)\n",
    "    kernel_results = [pool.apply_async(aggregate_grouped_values, ([mailbox[i]])) for i in range(__N_WORKERS__)]\n",
    "    values = [res.get(timeout=10) for res in kernel_results]\n",
    "    return display_aggregate(values,'Suburb', 'Number')\n",
    "\n",
    "#None of the code above will work UNTIL THIS IS EXECUTED\n",
    "if __name__ == '__main__':   \n",
    "    #Create the pool now to not waste time re-creating the thread pool\n",
    "    pool = Pool(processes=__N_WORKERS__)\n",
    "    \n",
    "%time task_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.terminate() ## Terminate the pool to prevent leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
